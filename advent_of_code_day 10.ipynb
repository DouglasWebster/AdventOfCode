{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advent of Code #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 10: Syntax Scoring ##\n",
    "\n",
    "You ask the submarine to determine the best route out of the deep-sea cave, but it only replies:\n",
    "\n",
    "Syntax error in navigation subsystem on line: all of them\n",
    "\n",
    "**All of them?!** The damage is worse than you thought. You bring up a copy of the navigation subsystem (your puzzle input).\n",
    "\n",
    "The navigation subsystem syntax is made of several lines containing **chunks.** There are one or more chunks on each line, and chunks contain zero or more other chunks. Adjacent chunks are not separated by any delimiter; if one chunk stops, the next chunk (if any) can immediately start. Every chunk must **open** and **close** with one of four legal pairs of matching characters:\n",
    "\n",
    "    If a chunk opens with (, it must close with ).\n",
    "    If a chunk opens with [, it must close with ].\n",
    "    If a chunk opens with {, it must close with }.\n",
    "    If a chunk opens with <, it must close with >.\n",
    "\n",
    "So, () is a legal chunk that contains no other chunks, as is []. More complex but valid chunks include ([]), {()()()}, <([{}])>, [<>({}){}[([])<>]], and even (((((((((()))))))))).\n",
    "\n",
    "Some lines are **incomplete,** but others are **corrupted.** Find and discard the corrupted lines first.\n",
    "\n",
    "A corrupted line is one where a chunk closes with the wrong character - that is, where the characters it opens and closes with do not form one of the four legal pairs listed above.\n",
    "\n",
    "Examples of corrupted chunks include `(]`, `{()()()>`, `(((()))}`, and `<([]){()}[{}])`. Such a chunk can appear anywhere within a line, and its presence causes the whole line to be considered corrupted.\n",
    "\n",
    "For example, consider the following navigation subsystem:\n",
    "```\n",
    "[({(<(())[]>[[{[]{<()<>>\n",
    "[(()[<>])]({[<{<<[]>>(\n",
    "{([(<{}[<>[]}>{[]{[(<()>\n",
    "(((({<>}<{<{<>}{[]{[]{}\n",
    "[[<[([]))<([[{}[[()]]]\n",
    "[{[{({}]{}}([{[{{{}}([]\n",
    "{<[[]]>}<{[{[{[]{()[[[]\n",
    "[<(<(<(<{}))><([]([]()\n",
    "<{([([[(<>()){}]>(<<{{\n",
    "<{([{{}}[<[[[<>{}]]]>[]]\n",
    "```\n",
    "Some of the lines aren't corrupted, just incomplete; you can ignore these lines for now. The remaining five lines are corrupted:\n",
    "\n",
    "    {([(<{}[<>[]}>{[]{[(<()> - Expected ], but found } instead.\n",
    "    [[<[([]))<([[{}[[()]]] - Expected ], but found ) instead.\n",
    "    [{[{({}]{}}([{[{{{}}([] - Expected ), but found ] instead.\n",
    "    [<(<(<(<{}))><([]([]() - Expected >, but found ) instead.\n",
    "    <{([([[(<>()){}]>(<<{{ - Expected ], but found > instead.\n",
    "\n",
    "Stop at the first incorrect closing character on each corrupted line.\n",
    "\n",
    "Did you know that syntax checkers actually have contests to see who can get the high score for syntax errors in a file? It's true! To calculate the syntax error score for a line, take the **first illegal character** on the line and look it up in the following table:\n",
    "\n",
    "    ): 3 points.\n",
    "    ]: 57 points.\n",
    "    }: 1197 points.\n",
    "    >: 25137 points.\n",
    "\n",
    "In the above example, an illegal ) was found twice (2*3 = **6** points), an illegal ] was found once (**57** points), an illegal } was found once (**1197** points), and an illegal > was found once (**25137** points). So, the total syntax error score for this file is 6+57+1197+25137 = **26397** points!\n",
    "\n",
    "Find the first illegal character in each corrupted line of the navigation subsystem. **What is the total syntax error score for those errors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy ###\n",
    "Should be solvable with a simple stack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_data = np.genfromtxt('data/syntax.dat', dtype=np.str0, delimiter='\\n')\n",
    "test_data = np.array(['[({(<(())[]>[[{[]{<()<>>', \\\n",
    "                        '[(()[<>])]({[<{<<[]>>(', \\\n",
    "                        '{([(<{}[<>[]}>{[]{[(<()>', \\\n",
    "                        '(((({<>}<{<{<>}{[]{[]{}' , \\\n",
    "                        '[[<[([]))<([[{}[[()]]]', \\\n",
    "                        '[{[{({}]{}}([{[{{{}}([]', \\\n",
    "                        '{<[[]]>}<{[{[{[]{()[[[]', \\\n",
    "                        '[<(<(<(<{}))><([]([]()', \\\n",
    "                        '<{([([[(<>()){}]>(<<{{', \\\n",
    "                        '<{([{{}}[<[[[<>{}]]]>[]]'])\n",
    "                        \n",
    "open_delimiters = ['(', '[', '{', '<']\n",
    "closing_delimiters = [')',']', '}', '>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[']', ')', ']', '>', ')', ']', ']', ')', '>', '}', '>', '>', ']', ']', ')', '}', '}', ']', ')', '>', ')', ']', ')', ')', ']', ']', '>', '>', '>', ']', '>', '}', ']', '>', '}', '>', ')', ')', '>', '>', ']', ')', '>', ')', ']'] give 358737 points\n"
     ]
    }
   ],
   "source": [
    "\n",
    "closing_vals = {\")\" : 3, \"]\" : 57, \"}\" : 1197, \">\": 25137 }\n",
    "\n",
    "line_to_check = \"\"\n",
    "bad_closing = []\n",
    "\n",
    "def check_line(line: str):\n",
    "    delimiter_stack = []\n",
    "    for i, c in enumerate(line):\n",
    "        if c in open_delimiters:\n",
    "            delimiter_stack.append(c)\n",
    "        else:\n",
    "            opening_del = delimiter_stack.pop()\n",
    "            del_index = open_delimiters.index(opening_del)\n",
    "            closing_del = closing_delimiters[del_index]\n",
    "            if c != closing_del:\n",
    "                bad_closing.append(c)\n",
    "                return\n",
    "    return\n",
    "\n",
    "\n",
    "for line in problem_data:\n",
    "    check_line(line)\n",
    "\n",
    "points = 0\n",
    "for x in bad_closing:\n",
    "    points += closing_vals[x]\n",
    "\n",
    "print(f'{bad_closing} give {points} points')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Part Two ##\n",
    "\n",
    "Now, discard the corrupted lines. The remaining lines are **incomplete.**\n",
    "\n",
    "Incomplete lines don't have any incorrect characters - instead, they're missing some closing characters at the end of the line. To repair the navigation subsystem, you just need to figure out the **sequence of closing characters** that complete all open chunks in the line.\n",
    "\n",
    "You can only use closing characters (), ], }, or >), and you must add them in the correct order so that only legal pairs are formed and all chunks end up closed.\n",
    "\n",
    "In the example above, there are five incomplete lines:\n",
    "\n",
    "    [({(<(())[]>[[{[]{<()<>> - Complete by adding }}]])})].\n",
    "    [(()[<>])]({[<{<<[]>>( - Complete by adding )}>]}).\n",
    "    (((({<>}<{<{<>}{[]{[]{} - Complete by adding }}>}>)))).\n",
    "    {<[[]]>}<{[{[{[]{()[[[] - Complete by adding ]]}}]}]}>.\n",
    "    <{([{{}}[<[[[<>{}]]]>[]] - Complete by adding ])}>.\n",
    "\n",
    "Did you know that autocomplete tools also have contests? It's true! The score is determined by considering the completion string character-by-character. Start with a total score of 0. Then, for each character, multiply the total score by 5 and then increase the total score by the point value given for the character in the following table:\n",
    "\n",
    "    ): 1 point.\n",
    "    ]: 2 points.\n",
    "    }: 3 points.\n",
    "    >: 4 points.\n",
    "\n",
    "So, the last completion string above - ])}> - would be scored as follows:\n",
    "\n",
    "    Start with a total score of 0.\n",
    "    Multiply the total score by 5 to get 0, then add the value of ] (2) to get a new total score of 2.\n",
    "    Multiply the total score by 5 to get 10, then add the value of ) (1) to get a new total score of 11.\n",
    "    Multiply the total score by 5 to get 55, then add the value of } (3) to get a new total score of 58.\n",
    "    Multiply the total score by 5 to get 290, then add the value of > (4) to get a new total score of 294.\n",
    "\n",
    "The five lines' completion strings have total scores as follows:\n",
    "\n",
    "    }}]])})] - 288957 total points.\n",
    "    )}>]}) - 5566 total points.\n",
    "    }}>}>)))) - 1480781 total points.\n",
    "    ]]}}]}]}> - 995444 total points.\n",
    "    ])}> - 294 total points.\n",
    "\n",
    "Autocomplete tools are an odd bunch: the winner is found by *sorting** all of the scores and then taking the **middle** score. (There will always be an odd number of scores to consider.) In this example, the middle score is **288957** because there are the same number of scores smaller and larger than it.\n",
    "\n",
    "Find the completion string for each incomplete line, score the completion strings, and sort the scores. **What is the middle score?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy ###\n",
    "Should be pretty much like part 1 but this time we need to save any stack that isn't empty when a good result occurs and walk back through the stack completeing the delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Middle score is 4329504793\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "closing_vals = {\")\" : 1, \"]\" : 2, \"}\" : 3, \">\": 4 }\n",
    "\n",
    "line_to_check = \"\"\n",
    "bad_closing = []\n",
    "\n",
    "def check_line(line: str):\n",
    "    delimiter_stack = []\n",
    "    for i, c in enumerate(line):\n",
    "        if c in open_delimiters:\n",
    "            delimiter_stack.append(c)\n",
    "        else:\n",
    "            opening_del = delimiter_stack.pop()\n",
    "            del_index = open_delimiters.index(opening_del)\n",
    "            closing_del = closing_delimiters[del_index]\n",
    "            if c != closing_del:\n",
    "                bad_closing.append(c)\n",
    "                return []\n",
    "    return delimiter_stack\n",
    "\n",
    "incomplete_closings = []\n",
    "incomplete_values = []\n",
    "for line in problem_data:\n",
    "    remaining_openings = check_line(line)\n",
    "    if len(remaining_openings) > 0:\n",
    "        score = 0\n",
    "        closings = []\n",
    "        remaining_openings = remaining_openings[::-1]\n",
    "        for x in remaining_openings:\n",
    "            closing = (closing_delimiters[open_delimiters.index(x)])\n",
    "            closings.append(closing)\n",
    "            score = 5 * score + closing_vals[closing]\n",
    "        incomplete_closings.append(closings)\n",
    "        incomplete_values.append(score)\n",
    "\n",
    "print(f'Middle score is {statistics.median(incomplete_values)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93cec69c38bad84d389aeaa7f91e1905006bdd97badbe8bc35fb9b6e193eb61c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('AdventOfCode': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
